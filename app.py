import streamlit as st
import requests
import pandas as pd
import google.generativeai as genai
import time
import json
import urllib.parse
import xml.etree.ElementTree as ET
import re
import jieba 
from wordcloud import WordCloud 
import matplotlib.pyplot as plt 
import os

# --- 1. è¨­å®šé é¢ ---
st.set_page_config(page_title="æˆ¿å¸‚è¼¿æƒ…é›·é” AI ç‰ˆ", page_icon="ğŸ ", layout="wide")

# --- 2. å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    if 'valid_api_key' not in st.session_state:
        st.session_state.valid_api_key = st.secrets.get("GEMINI_API_KEY", None)
    if not st.session_state.valid_api_key:
        user_input_key = st.text_input("è«‹è¼¸å…¥ Google Gemini API Key", type="password")
        if st.button("âœ… é©—è­‰ä¸¦è¨­å®š", type="primary"):
            if not user_input_key: st.error("âŒ è«‹è¼¸å…¥å…§å®¹")
            else:
                try:
                    genai.configure(api_key=user_input_key)
                    list(genai.list_models()) 
                    st.session_state.valid_api_key = user_input_key
                    st.success("é©—è­‰æˆåŠŸï¼")
                    time.sleep(1)
                    st.rerun()
                except Exception as e: st.error(f"âŒ ç„¡æ•ˆ: {e}")
    else:
        st.success("âœ… API Key å·²è¨­å®š")
        if st.secrets.get("GEMINI_API_KEY") is None:
            if st.button("ğŸ”„ æ¸…é™¤/æ›´æ› Key"):
                st.session_state.valid_api_key = None
                st.rerun()
    st.divider()
    force_demo_ai = st.checkbox("ğŸ”§ å¼·åˆ¶ä½¿ç”¨æ¨¡æ“¬ AI çµæœ (Demoç”¨)", value=False)

# --- æ¨¡å‹æ™ºæ…§é¸æ“‡ ---
def get_best_model_name(api_key):
    try:
        genai.configure(api_key=api_key)
        all_models = list(genai.list_models())
        text_models = [m.name for m in all_models if 'generateContent' in m.supported_generation_methods]
        for m in text_models:
            if 'gemini-1.5-flash' in m: return m
        for m in text_models:
            if 'gemini-pro' in m: return m
        if text_models: return text_models[0]
        return "gemini-pro"
    except: return "gemini-pro"

# --- é»‘åå–® ---
BLOCKED_FORUM_IDS = ["f=214", "f=260", "f=261", "f=565", "f=168", "f=738", "f=61", "f=37", "f=320"]
def is_blocked_link(link):
    if not link: return True
    for fid in BLOCKED_FORUM_IDS:
        if fid in link: return True
    return False

# --- Topic ID ---
def get_topic_id(link):
    match = re.search(r't=(\d+)', link)
    if match: return int(match.group(1))
    return 0

# --- [å¼·åŠ›ä¿®å¾©] è‡ªå‹•ä¸‹è¼‰ä¸­æ–‡å­—å‹ ---
def download_font():
    # æ”¹ç”¨ "æ–‡æ³‰é©›å¾®ç±³é»‘"ï¼Œé€™æ˜¯ä¸€å€‹éå¸¸ç©©å®šä¸”å¸¸ç”¨çš„é–‹æºä¸­æ–‡å­—å‹
    font_filename = "WenQuanYiMicroHei.ttf"
    font_url = "https://github.com/anthonyhilyard/GitHub-Chinese-Fonts/raw/master/WenQuanYiMicroHei.ttf"
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if os.path.exists(font_filename):
        # å¦‚æœæª”æ¡ˆå¤ªå° (å°æ–¼ 1MB)ï¼Œä»£è¡¨ä¸Šæ¬¡ä¸‹è¼‰å¤±æ•—æ˜¯å£æª”ï¼Œåˆªæ‰é‡æŠ“
        if os.path.getsize(font_filename) < 1000000:
            os.remove(font_filename)
        else:
            return font_filename # æª”æ¡ˆæ­£å¸¸ï¼Œç›´æ¥å›å‚³
    
    # é–‹å§‹ä¸‹è¼‰
    try:
        with st.spinner("æ­£åœ¨ä¸‹è¼‰ä¸­æ–‡å­—å‹è³‡æº (é¦–æ¬¡éœ€æ™‚ç´„ 10 ç§’)..."):
            response = requests.get(font_url, timeout=30)
            if response.status_code == 200:
                with open(font_filename, "wb") as f:
                    f.write(response.content)
                return font_filename
            else:
                st.warning("å­—å‹ä¸‹è¼‰é€£ç·šå¤±æ•—ï¼Œæ–‡å­—é›²å°‡ç„¡æ³•é¡¯ç¤ºä¸­æ–‡ã€‚")
                return None
    except Exception as e:
        st.warning(f"å­—å‹ä¸‹è¼‰éŒ¯èª¤: {e}")
        return None

# --- ç”¢ç”Ÿæ–‡å­—é›² ---
def generate_wordcloud(titles_list):
    text = " ".join(titles_list)
    # è¨­å®šåœç”¨è©
    stopwords = {
        "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "äºº", "éƒ½", "ä¸€å€‹", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "èªª", "è¦", "å»", "ä½ ",
        "æœƒ", "è‘—", "æ²’æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "é€™", "è«‹å•", "è«‹ç›Š", "è¨è«–", "åˆ†äº«", "å•é¡Œ", "å¤§å®¶", "çŸ¥é“", "Mobile01",
        "ä»€éº¼", "æ€éº¼", "å¯ä»¥", "çœŸçš„", "å› ç‚º", "æ‰€ä»¥", "å¦‚æœ", "ä½†æ˜¯", "æ¯”è¼ƒ", "è¦ºå¾—", "ç¾åœ¨", "é‚„æ˜¯", "æœ‰æ²’æœ‰", "æ–‡ç« ",
        "æ¨™é¡Œ", "é€£çµ", "ä¾†æº", "ç™¼å¸ƒæ™‚é–“"
    }
    
    try:
        words = jieba.cut(text)
        filtered_words = [word for word in words if word not in stopwords and len(word) > 1]
        text_clean = " ".join(filtered_words)
        
        if not text_clean.strip(): return None 

        # å–å¾—å­—å‹è·¯å¾‘
        font_path = download_font()
        
        if font_path:
            wc = WordCloud(
                font_path=font_path, # æŒ‡å®šä¸­æ–‡å­—å‹
                background_color="white",
                width=800, height=400,
                max_words=80, 
                colormap="viridis",
                font_step=2,
                min_font_size=10
            ).generate(text_clean)
        else:
            # æ²’å­—å‹å°±ç”¨é è¨­ (æœƒè®Šæ–¹å¡Šï¼Œä½†è‡³å°‘æœ‰åœ–)
            wc = WordCloud(
                background_color="white",
                width=800, height=400,
                max_words=80
            ).generate(text_clean)
        
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wc, interpolation="bilinear")
        ax.axis("off")
        return fig

    except Exception as e:
        print(f"æ–‡å­—é›²ç¹ªè£½å¤±æ•—: {e}") 
        return None

# --- 3. æœå°‹å‡½æ•¸ ---
def search_mobile01_via_google(keyword):
    if not keyword: keyword = "å°åŒ— æˆ¿ç”¢"
    real_estate_terms = "é å”® OR å»ºæ¡ˆ OR æˆ¿åƒ¹ OR åªæ•¸ OR æ ¼å±€ OR å…¬å¯“ OR å¤§æ¨“ OR è±ªå®… OR ç½®ç”¢ OR è²·æˆ¿"
    search_query = f"{keyword} ({real_estate_terms}) site:mobile01.com when:1y"
    encoded_query = urllib.parse.quote(search_query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    try:
        response = requests.get(rss_url, timeout=10)
        root = ET.fromstring(response.content)
        articles = []
        items = root.findall('.//item')
        for item in items[:50]:
            title = item.find('title').text if item.find('title') is not None else "ç„¡æ¨™é¡Œ"
            link = item.find('link').text if item.find('link') is not None else "#"
            pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
            title = title.replace("- Mobile01", "").strip()
            if is_blocked_link(link): continue
            tid = get_topic_id(link)
            articles.append({"æ¨™é¡Œ": title, "é€£çµ": link, "ä¾†æº": "Mobile01", "ç™¼å¸ƒæ™‚é–“": pub_date, "topic_id": tid})
        articles.sort(key=lambda x: x['topic_id'], reverse=True)
        return articles[:15] 
    except Exception as e:
        st.error(f"æœå°‹éŒ¯èª¤: {e}"); return []

def get_demo_data():
    return [{"æ¨™é¡Œ": "åŒ—å£«ç§‘é å”®å±‹é–‹åƒ¹ç ´ç™¾è¬åˆç†å—ï¼Ÿå¿ƒå¾ˆç´¯", "é€£çµ": "https://www.mobile01.com/t=999"},
            {"æ¨™é¡Œ": "è«‹å• XX å»ºæ¡ˆçš„æ–½å·¥å“è³ªå¦‚ä½•ï¼Ÿæœ‰æ¼æ°´æ¡ˆä¾‹å—", "é€£çµ": "https://www.mobile01.com/t=888"},
            {"æ¨™é¡Œ": "åˆ†äº«ï¼šçµ‚æ–¼ç°½ç´„äº†ï¼æ ¼å±€çœŸçš„å¾ˆæ£’ï¼Œä½†åƒ¹æ ¼ç¡¬", "é€£çµ": "https://www.mobile01.com/t=777"},
            {"æ¨™é¡Œ": "ç¾åœ¨é€²å ´åŒ—å£«ç§‘æ˜¯ä¸æ˜¯é«˜é»ï¼Ÿæ€•è¢«å¥—ç‰¢", "é€£çµ": "https://www.mobile01.com/t=666"},
            {"æ¨™é¡Œ": "ä¿¡ç¾©å€èˆŠå…¬å¯“ vs åŒ—å£«ç§‘æ–°æˆå±‹ æ€éº¼é¸ï¼Ÿ", "é€£çµ": "https://www.mobile01.com/t=555"}]

# --- 4. AI åˆ†æ ---
def analyze_with_gemini(df, use_fake=False):
    current_key = st.session_state.valid_api_key
    is_simulated = use_fake or (not current_key)

    if is_simulated:
        time.sleep(1)
        fake_summary = f"ã€æ¨¡æ“¬å¿«å ±ã€‘é‡å°æœ¬æ¬¡æœå°‹çµæœï¼Œæ•´é«”å¸‚å ´æ°›åœåå‘è§€æœ›èˆ‡ç„¦æ…®ã€‚ç¶²å‹è¨è«–ç„¦é»é›†ä¸­åœ¨ã€Œåƒ¹æ ¼éé«˜ã€èˆ‡ã€Œå»ºå•†å“ç‰Œä¿¡ä»»åº¦ã€ã€‚éƒ¨åˆ†è¨è«–æåŠã€Œæ–½å·¥å“è³ªã€èˆ‡ã€Œæ¼æ°´ã€ç–‘æ…®ï¼Œé¡¯ç¤ºè²·æ–¹å°é¢¨éšªæ„è­˜æé«˜ã€‚"
        demo_sentiments = ["ç„¦æ…®", "è² é¢", "æ­£é¢", "è§€æœ›", "ä¸­ç«‹"] * 3
        demo_keywords = ["åƒ¹æ ¼éé«˜", "æ¼æ°´ç–‘æ…®", "æ ¼å±€æ–¹æ­£", "é«˜é»å¥—ç‰¢", "é‡åŠƒå€ç™¼å±•"] * 3
        df['AIæƒ…ç·’'] = demo_sentiments[:len(df)]
        df['é—œéµé‡é»'] = demo_keywords[:len(df)]
        return df, fake_summary, None, True
    
    try:
        genai.configure(api_key=current_key)
        best_model = get_best_model_name(current_key)
        model = genai.GenerativeModel(best_model) 
        titles_text = "\n".join([f"{i+1}. {t}" for i, t in enumerate(df['æ¨™é¡Œ'].tolist())])
        
        prompt = f"""
        ä½ æ˜¯å°ˆæ¥­çš„æˆ¿åœ°ç”¢è¼¿æƒ…åˆ†æå¸«ã€‚è«‹é–±è®€ä»¥ä¸‹ Mobile01 è¨è«–å€çš„æ¨™é¡Œï¼š
        {titles_text}
        
        è«‹åŸ·è¡Œå…©é …ä»»å‹™ï¼š
        ä»»å‹™ä¸€ï¼šæ’°å¯«ã€Œå¸‚å ´è¼¿æƒ…å¿«å ±ã€(ç´„ 3-5 å¥è©±)ã€‚ç¶œåˆåˆ†æé€™äº›æ¨™é¡Œåæ˜ å‡ºçš„æ•´é«”å¸‚å ´æƒ…ç·’ã€ç¶²å‹æœ€é—œæ³¨çš„ç†±é»è­°é¡Œã€‚
        ä»»å‹™äºŒï¼šé‡å°æ¯ä¸€å€‹æ¨™é¡Œé€²è¡Œè©³ç´°åˆ†æã€‚

        è«‹ç›´æ¥å›å‚³ä¸€å€‹ JSON æ ¼å¼çš„è³‡æ–™ï¼Œæ ¼å¼å¦‚ä¸‹ï¼ˆä¸è¦ Markdown æ¨™è¨˜ï¼‰ï¼š
        {{
            "summary_report": "åœ¨é€™è£¡å¡«å¯«ä½ çš„å¸‚å ´è¼¿æƒ…å¿«å ±å…§å®¹...",
            "details": [
                {{"sentiment": "æ­£é¢/è² é¢/ä¸­ç«‹/ç„¦æ…®/è§€æœ›", "keyword": "é—œéµå­—1, é—œéµå­—2"}}
            ]
        }}
        ç¢ºä¿ "details" åˆ—è¡¨çš„é•·åº¦èˆ‡è¼¸å…¥çš„æ¨™é¡Œæ•¸é‡å®Œå…¨ä¸€è‡´ã€‚
        """
        response = model.generate_content(prompt)
        clean_text = response.text.replace("```json", "").replace("```python", "").replace("```", "").strip()
        
        try:
            result_json = json.loads(clean_text)
            summary_report = result_json.get("summary_report", "AI ç„¡æ³•ç”¢ç”Ÿç¸½çµå ±å‘Šã€‚")
            details = result_json.get("details", [])
        except:
            summary_report = "AI å›å‚³æ ¼å¼ç•°å¸¸ï¼Œç„¡æ³•è§£æç¸½çµå ±å‘Šã€‚"
            details = []

        sentiments = [item.get('sentiment', 'æœªçŸ¥') for item in details]
        keywords = [item.get('keyword', 'ç„¡') for item in details]
        
        while len(sentiments) < len(df):
            sentiments.append("æœªçŸ¥"); keywords.append("ç„¡")
            
        df['AIæƒ…ç·’'] = sentiments[:len(df)]
        df['é—œéµé‡é»'] = keywords[:len(df)]
        return df, summary_report, None, False 
        
    except Exception as e:
        return df, "", str(e), False

# --- 5. ä¸»ç¨‹å¼ä»‹é¢ ---
st.title("ğŸ  æˆ¿å¸‚è¼¿æƒ…é›·é” + AI æ´å¯Ÿ") 

if 'data' not in st.session_state: st.session_state.data = []
if 'analyzed_data' not in st.session_state: st.session_state.analyzed_data = None
if 'summary_report' not in st.session_state: st.session_state.summary_report = ""

st.write("### ğŸ” è¼¿æƒ…é—œéµå­—æœå°‹")
col_input, col_btn = st.columns([3, 1])
with col_input:
    keyword = st.text_input("è¼¸å…¥é—œéµå­— (ä¾‹å¦‚ï¼šåŒ—å£«ç§‘ã€é å”®å±‹ã€æŸæŸå»ºæ¡ˆ)", "åŒ—å£«ç§‘")
with col_btn:
    st.write(""); st.write("")
    if st.button("ğŸš€ æœå°‹æœ€æ–°è©±é¡Œ", type="primary"):
        with st.spinner(f'æ­£åœ¨è’é›†é—œæ–¼ã€Œ{keyword}ã€çš„æœ€æ–°è¨è«–...'):
            st.session_state.data = search_mobile01_via_google(keyword)
            st.session_state.analyzed_data = None
            st.session_state.summary_report = ""
            if not st.session_state.data: st.warning(f"æ‰¾ä¸åˆ°ç›¸é—œè¨è«–ã€‚")

if st.button("ğŸ“‚ è¼‰å…¥ç¯„ä¾‹è³‡æ–™ (Demo)", help="æœå°‹ä¸åˆ°æ™‚ä½¿ç”¨"):
    st.session_state.data = get_demo_data()
    st.session_state.analyzed_data = None 
    st.success("å·²è¼‰å…¥æ¨¡æ“¬æ•¸æ“šï¼")

# --- 6. é¡¯ç¤ºå…§å®¹å€ ---
if st.session_state.data:
    df = pd.DataFrame(st.session_state.data)
    st.divider()
    
    tab1, tab2 = st.tabs(["ğŸ“Š AI æ´å¯Ÿå ±å‘Š & æ–‡å­—é›²", "ğŸ“‹ åŸå§‹è©±é¡Œåˆ—è¡¨"])
    
    with tab2: 
        st.write(f"å…±è’é›† {len(df)} å‰‡æœ€æ–°è©±é¡Œ")
        st.dataframe(df[['æ¨™é¡Œ', 'é€£çµ']], 
                     column_config={"é€£çµ": st.column_config.LinkColumn("æ–‡ç« é€£çµ")},
                     use_container_width=True)
        st.info("ğŸ’¡ è«‹åˆ‡æ›åˆ°ã€ŒAI æ´å¯Ÿå ±å‘Šã€åˆ†é é€²è¡Œåˆ†æ")

    with tab1: 
        st.write("### ğŸ§  AI è¼¿æƒ…åˆ†æä¸­å¿ƒ")
        
        if st.session_state.analyzed_data is None: 
            if st.button("ğŸ¤– å•Ÿå‹• AI å…¨é¢è§£è®€ (åŒ…å«æ–‡å­—é›²)", type="primary"):
                with st.spinner("AI æ­£åœ¨é–±è®€æ¨™é¡Œã€ç”¢ç”Ÿæ‘˜è¦ä¸¦ç¹ªè£½æ–‡å­—é›²..."):
                    result_df, summary, error, is_sim = analyze_with_gemini(df, use_fake=force_demo_ai)
                    st.session_state.analyzed_data = result_df
                    st.session_state.summary_report = summary
                    st.session_state.is_simulated = is_sim
                    st.session_state.error_msg = error
                    st.rerun()
        
        if st.session_state.analyzed_data is not None:
            if st.session_state.is_simulated:
                st.warning("âš ï¸ ç›®å‰ç‚ºã€Œæ¨¡æ“¬æ¼”ç¤ºæ¨¡å¼ã€(ç„¡ API Key)")
            else:
                st.success("âœ… AI çœŸå¯¦åˆ†æå®Œæˆ")
            if st.session_state.error_msg: st.error(f"ç•°å¸¸: {st.session_state.error_msg}")
            
            st.markdown("""---""")
            st.subheader("ğŸ“ AI å¸‚å ´è¼¿æƒ…å¿«å ±")
            if st.session_state.summary_report:
                st.info(st.session_state.summary_report, icon="ğŸ’¡")
            
            st.markdown("""---""")
            col_wc, col_chart = st.columns([3, 2])
            
            with col_wc:
                st.subheader("â˜ï¸ è©±é¡Œç†±é»æ–‡å­—é›²")
                try:
                    wc_fig = generate_wordcloud(st.session_state.data[i]['æ¨™é¡Œ'] for i in range(len(st.session_state.data)))
                    if wc_fig:
                        st.pyplot(wc_fig)
                    else:
                        st.warning("æ–‡å­—é›²ç”¢ç”Ÿå¤±æ•— (å¯èƒ½å­—å‹ä¸‹è¼‰ä¸å®Œå…¨)ï¼Œä½†ä¸å½±éŸ¿å…¶ä»–åŠŸèƒ½ã€‚")
                except Exception as wc_error:
                     st.warning(f"æ–‡å­—é›²æš«æ™‚ç„¡æ³•é¡¯ç¤º: {wc_error}")

            with col_chart:
                st.subheader("ğŸ“ˆ æƒ…ç·’åˆ†ä½ˆæŒ‡æ¨™")
                st.bar_chart(st.session_state.analyzed_data['AIæƒ…ç·’'].value_counts())

            st.markdown("""---""")
            st.subheader("ğŸ” è©³ç´°åˆ†ææ•¸æ“š")
            with st.expander("é»æ“Šå±•é–‹æŸ¥çœ‹é€ç­†åˆ†æçµæœ"):
                st.dataframe(
                    st.session_state.analyzed_data[['é€£çµ', 'æ¨™é¡Œ', 'AIæƒ…ç·’', 'é—œéµé‡é»']], 
                    column_config={
                        "é€£çµ": st.column_config.LinkColumn("å‰å¾€"), 
                        "AIæƒ…ç·’": st.column_config.TextColumn("æƒ…ç·’"),
                    },
                    use_container_width=True
                )
else:
    st.info("ğŸ‘ˆ è«‹å…ˆåœ¨å·¦å´è¼¸å…¥é—œéµå­—ä¸¦æœå°‹")
